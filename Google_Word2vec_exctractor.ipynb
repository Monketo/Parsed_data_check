{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "READ_PATH=\"GoogleNews-vectors-negative300.csv\"\n",
    "WRITE_WORDS_PATH=\"google_words/google_words\"\n",
    "WRITE_VECTORS_PATH=\"google_words/google_vectors\"\n",
    "\n",
    "\n",
    "def load_samples_to_files(file_path,vectors_path,words_path,amount_of_words=1,parse_all=False,save_frequency=300000):\n",
    "    '''Helper function to load csv data into list with intent to serialize them later\n",
    "    We are doing that with the objective to visualize embedding matrix in tensorboard'''\n",
    "    \n",
    "    counter=0\n",
    "    words = []\n",
    "    vectors = []\n",
    "    with open(file_path,\"r\") as file:\n",
    "        \n",
    "        \n",
    "        for row in csv.reader(file):\n",
    "            counter+=1\n",
    "            try:\n",
    "                all_row = row[0].split()\n",
    "                word=all_row[0]\n",
    "                vecrot_rep=all_row[1:]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            if len(vecrot_rep)==300:    \n",
    "                words.append(word)\n",
    "                vectors.append(vecrot_rep) \n",
    "            if not parse_all:    \n",
    "                if counter == amount_of_words:\n",
    "                    return(words,vectors)\n",
    "                \n",
    "            if counter%17000 == 0:\n",
    "            \n",
    "                print(\"Currently {} words were traversed, current word {}\".format(counter,word))\n",
    "            \n",
    "            if counter%save_frequency == 0:\n",
    "                \n",
    "                FILE_PATH1=vectors_path+\"_\"+str(counter)+\".pkl\"\n",
    "                FILE_PATH2=words_path+\"_\"+str(counter)+\".pkl\"\n",
    "                \n",
    "                print(\"Checking consistency ...,\",len(words)==len(vectors))\n",
    "                print(\"Checking consistency of vector lengthes\",equal_length(vectors,300))\n",
    "                print(\"Saving files\")\n",
    "                print(\"Words will be loaded\",len(words))\n",
    "                \n",
    "                if not os.path.exists(FILE_PATH1):\n",
    "                    with open(FILE_PATH2,\"wb\") as file:\n",
    "                        pickle.dump(words,file)\n",
    "                        \n",
    "                    with open(FILE_PATH1,\"wb\") as file2:\n",
    "                        pickle.dump(vectors,file2)\n",
    "                        \n",
    "                vectors=[]\n",
    "                words=[]\n",
    "                   \n",
    "                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently 17000 words were traversed, current word flashing\n",
      "Currently 34000 words were traversed, current word wireless_carriers\n",
      "Currently 51000 words were traversed, current word Simeon\n",
      "Currently 68000 words were traversed, current word sprucing\n",
      "Currently 85000 words were traversed, current word ###F\n",
      "Currently 102000 words were traversed, current word Christopher_S._Rugaber\n",
      "Currently 119000 words were traversed, current word Dundee_Crown\n",
      "Currently 136000 words were traversed, current word dazzling_array\n",
      "Currently 153000 words were traversed, current word Sergei_Bobrovsky\n",
      "Currently 170000 words were traversed, current word od\n",
      "Currently 187000 words were traversed, current word Whaler\n",
      "Currently 204000 words were traversed, current word Ulrich_Busch\n",
      "Currently 221000 words were traversed, current word boneless_beef\n",
      "Currently 238000 words were traversed, current word Oien\n",
      "Currently 255000 words were traversed, current word Gardere\n",
      "Currently 272000 words were traversed, current word Foxwood\n",
      "Currently 289000 words were traversed, current word bounteous\n",
      "Checking consistency ..., True\n",
      "Checking consistency of vector lengthes True\n",
      "Saving files\n",
      "Words will be loaded 299834\n",
      "Currently 306000 words were traversed, current word Rotuma\n",
      "Currently 323000 words were traversed, current word Infraero\n",
      "Currently 340000 words were traversed, current word grossers\n",
      "Currently 357000 words were traversed, current word Shipwrecks\n",
      "Currently 374000 words were traversed, current word PokerStars_Full_Tilt\n",
      "Currently 391000 words were traversed, current word Rashidiya\n",
      "Currently 408000 words were traversed, current word pleura\n",
      "Currently 425000 words were traversed, current word Higher_Education_Opportunity\n",
      "Currently 442000 words were traversed, current word PERRIS_Calif.\n",
      "Currently 459000 words were traversed, current word Morayfield\n",
      "Currently 476000 words were traversed, current word Metallica_drummer\n",
      "Currently 493000 words were traversed, current word Genzyme_NASDAQ_GENZ\n",
      "Currently 510000 words were traversed, current word cubic_zirconium\n",
      "Currently 527000 words were traversed, current word covert_uranium_enrichment\n",
      "Currently 544000 words were traversed, current word Glory_Glory\n",
      "Currently 561000 words were traversed, current word tangelos\n",
      "Currently 578000 words were traversed, current word SSDS\n",
      "Currently 595000 words were traversed, current word LONGVIEW_TX_KLTV\n",
      "Checking consistency ..., True\n",
      "Checking consistency of vector lengthes True\n",
      "Saving files\n",
      "Words will be loaded 299751\n",
      "Currently 612000 words were traversed, current word PRNewswire_FirstCall_Amdocs\n",
      "Currently 629000 words were traversed, current word Mireya_Castilla_Corporate\n",
      "Currently 646000 words were traversed, current word Islam_holiest_shrine\n",
      "Currently 663000 words were traversed, current word tow_strap\n",
      "Currently 680000 words were traversed, current word ##.#oz\n",
      "Currently 697000 words were traversed, current word Serrill\n",
      "Currently 714000 words were traversed, current word Biz_Brain\n",
      "Currently 731000 words were traversed, current word ##am_##.##pm\n",
      "Currently 748000 words were traversed, current word WEST_NEWBURY\n",
      "Currently 765000 words were traversed, current word Druin\n",
      "Currently 782000 words were traversed, current word Etlaala\n",
      "Currently 799000 words were traversed, current word Abbad\n",
      "Currently 816000 words were traversed, current word X_ray_mammography\n",
      "Currently 833000 words were traversed, current word QD3\n",
      "Currently 850000 words were traversed, current word Meelia\n",
      "Currently 867000 words were traversed, current word Caneland\n",
      "Currently 884000 words were traversed, current word National_Foreclosure_Mitigation\n",
      "Checking consistency ..., True\n",
      "Checking consistency of vector lengthes True\n",
      "Saving files\n",
      "Words will be loaded 299749\n",
      "Currently 901000 words were traversed, current word Choi_Kyung_ju\n",
      "Currently 918000 words were traversed, current word afc.com\n",
      "Currently 935000 words were traversed, current word Chysler\n",
      "Currently 952000 words were traversed, current word Service_Provisioning_Platform\n",
      "Currently 969000 words were traversed, current word MoqiZone\n",
      "Currently 986000 words were traversed, current word INCLUDED_IN\n",
      "Currently 1003000 words were traversed, current word NHLPA_Goals\n",
      "Currently 1020000 words were traversed, current word incl._GST\n",
      "Currently 1037000 words were traversed, current word shifting_tectonic_plates\n",
      "Currently 1054000 words were traversed, current word Advisor_CTA\n",
      "Currently 1071000 words were traversed, current word Stirrett\n",
      "Currently 1088000 words were traversed, current word Heuertz\n",
      "Currently 1105000 words were traversed, current word Nanette_Fabray\n",
      "Currently 1122000 words were traversed, current word Ros├írio\n",
      "Currently 1139000 words were traversed, current word Shuttle_Hurdle\n",
      "Currently 1156000 words were traversed, current word KTCU\n",
      "Currently 1173000 words were traversed, current word Protected_Cell\n",
      "Currently 1190000 words were traversed, current word coach_Sam_Rutigliano\n",
      "Checking consistency ..., True\n",
      "Checking consistency of vector lengthes True\n",
      "Saving files\n",
      "Words will be loaded 299765\n",
      "Currently 1207000 words were traversed, current word hereinafter_collectively\n",
      "Currently 1224000 words were traversed, current word http://www.auctionbytes.com/cab/abn/y##/m##/i##/s##\n"
     ]
    }
   ],
   "source": [
    "load_samples_to_files(READ_PATH,WRITE_VECTORS_PATH,WRITE_WORDS_PATH,parse_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serialize_embedding(vectors):\n",
    "    \"\"\"Name of the function is self-explanatory\"\"\"\n",
    "    \n",
    "    with open(\"vectors.pkl\",\"w\") as file:\n",
    "        pickle.dump(vectors,file=file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serialize_words(words):\n",
    "    \"\"\"Name of the function is self-explanatory\"\"\"\n",
    "    with open(\"words.tsv\",\"w\") as file:\n",
    "        for word in words:\n",
    "            print(words,file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equal_length(array,length):\n",
    "    \"Check the consistency of loaded arrays\"\n",
    "    return all(len(arr)==length for arr in array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal_length(vectors,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n"
     ]
    }
   ],
   "source": [
    "with open(READ_PATH,\"r\") as file:\n",
    "    for row in csv.reader(file):\n",
    "        print(row[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
